{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can download the pretrained weights from the following link \n",
    "#https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "#or you can find the detailed documentation https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "\n",
    "from keras.models import model_from_json\n",
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    #img = load_img(image_path, target_size=(224, 224))\n",
    "    #img = img_to_array(img)\n",
    "    #img = np.expand_dims(image_path, axis=0)\n",
    "    img = preprocess_input(image_path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "epsilon = 0.40\n",
    "images=glob.glob('dataset/*.jpg')\n",
    "def verifyFace(img1):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n",
    "    cosine=[]\n",
    "    faces=[]\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        img2=cv2.imread(images[i])\n",
    "        img2=cv2.resize(img2, (width, height))\n",
    "        img2.shape\n",
    "        img2=img2.reshape(1,224,224,3)\n",
    "        img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "    \n",
    "        cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "        euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n",
    "    \n",
    "        #print(\"Cosine similarity: \",cosine_similarity)\n",
    "        #print(\"Euclidean distance: \",euclidean_distance)\n",
    "        \n",
    "        \n",
    "        if(cosine_similarity < epsilon):\n",
    "            cosine.append(cosine_similarity)\n",
    "            faces.append(images[i])\n",
    "            \n",
    "            #return cosine_similarity\n",
    "            #print(\"verified... they are same person\")\n",
    "        else:\n",
    "            return cosine_similarity\n",
    "            #print(\"unverified! they are not same person!\")\n",
    "        if len(cosine) !=0:\n",
    "            pos = cosine.index(min(cosine))\n",
    "            #print(pos)\n",
    "            res = faces[pos]\n",
    "        else:\n",
    "            res =  0\n",
    "    \n",
    "        print(\"-----------------------------------------\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "verifyFace() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-10558abdd2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mface_resize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mface_resize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_resize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverifyFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_resize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrrec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: verifyFace() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "(width, height) = (224, 224)\n",
    "img1=cv2.imread('dataset/321.jpeg')\n",
    "img1=cv2.resize(img1, (width, height))\n",
    "img1.shape\n",
    "img1=img1.reshape(1,224,224,3)\n",
    "video_capture = cv2.VideoCapture(1)\n",
    "\n",
    "                 \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5\n",
    "        #minSize=(160, 160),\n",
    "        #flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        rrec = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w] \n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        face_resize.shape\n",
    "        face_resize = face_resize.reshape(1,224,224,3)\n",
    "        text = verifyFace(face_resize, img1)\n",
    "        text=str(text)\n",
    "        cv2.putText(rrec,text,(100,100), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),1,cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.383620023727417, 0.2354246973991394, 0.3948202133178711, 0.5790610909461975, 0.38421642780303955, 0.3970634937286377, 0.2206088900566101, 0.45228129625320435, 0.38255053758621216, 0.1445363163948059, 0.39763033390045166, 0.07173305749893188, 0.3919040560722351, 0.4087962508201599, 0.3914046287536621]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n",
      "[0.3957972526550293, 0.2412150502204895, 0.40790945291519165, 0.570666640996933, 0.37744808197021484, 0.40562981367111206, 0.21573621034622192, 0.4730333089828491, 0.3970620632171631, 0.14061444997787476, 0.4067888855934143, 0.06627082824707031, 0.4022824764251709, 0.42716550827026367, 0.4068108797073364]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n",
      "[0.3942551016807556, 0.248488187789917, 0.39966678619384766, 0.5838851630687714, 0.37865591049194336, 0.3997182846069336, 0.2002044916152954, 0.46877360343933105, 0.39203691482543945, 0.13833516836166382, 0.4004455804824829, 0.07734066247940063, 0.396098256111145, 0.42317700386047363, 0.4010552763938904]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n",
      "[0.3979548215866089, 0.25129640102386475, 0.4060819149017334, 0.594173401594162, 0.3811873197555542, 0.4023485779762268, 0.2037559151649475, 0.47107988595962524, 0.3964229226112366, 0.14294904470443726, 0.40382492542266846, 0.06906259059906006, 0.3972631096839905, 0.4273359179496765, 0.4053608179092407]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n",
      "[0.39127475023269653, 0.23792052268981934, 0.3982534408569336, 0.6002520620822906, 0.3742942810058594, 0.3955047130584717, 0.19719862937927246, 0.4664434790611267, 0.3883829712867737, 0.1361929178237915, 0.3964838981628418, 0.07671427726745605, 0.3915322422981262, 0.419727087020874, 0.3973022699356079]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n",
      "[0.40551161766052246, 0.2549476623535156, 0.41422218084335327, 0.5701552927494049, 0.3679986596107483, 0.4005522131919861, 0.20441043376922607, 0.506857305765152, 0.40938740968704224, 0.1344727873802185, 0.40152549743652344, 0.051029980182647705, 0.3973853588104248, 0.4519197344779968, 0.42221224308013916]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "idx=0\n",
    "                 \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(224, 224),\n",
    "        #flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        rrec = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w] \n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        #face_resize.shape\n",
    "        face_resize = face_resize.reshape(1,224,224,3)\n",
    "        text = verifyFace(face_resize)\n",
    "        text=str(text)\n",
    "        if text=='donot':\n",
    "            \n",
    "            write_name = 'dataset/new face.'+str(idx)+'.jpg'\n",
    "            cv2.imwrite(write_name, face)\n",
    "            idx+=1\n",
    "        cv2.putText(rrec,text,(100,100), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),1,cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "epsilon=0.2\n",
    "images=glob.glob('dataset/*.jpg')\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "def verifyFace(img1):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n",
    "    \n",
    "    cosines=[]\n",
    "    faces=[]\n",
    "    for i in range(len(images)):\n",
    "        img2=cv2.imread(images[i])\n",
    "        img2=cv2.resize(img2, (width, height))\n",
    "        img2.shape\n",
    "        img2=img2.reshape(1,224,224,3)\n",
    "        img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "\n",
    "        cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "        cosines.append(cosine_similarity)\n",
    "        \n",
    "        #print(cosines)\n",
    "        #print(images)\n",
    "        \n",
    "            \n",
    "    #print(minimum)\n",
    "    minimum = cosines.index(min(cosines))\n",
    "    #print(cosines[minimum])\n",
    "    print(cosines)\n",
    "    print(images)\n",
    "    #print('--------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    if float(cosines[minimum])<0.2:\n",
    "        return images[minimum]\n",
    "            \n",
    "    if float(cosines[minimum])>0.2:\n",
    "        return 'donot'\n",
    "    \n",
    "#return res\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    #if(cosine_similarity < epsilon):\n",
    "     #   print(cosine_similarity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4188094139099121, 0.2767372131347656, 0.4100835919380188, 0.5538826882839203, 0.38239938020706177, 0.39794373512268066, 0.18326318264007568, 0.5183283686637878, 0.4214453101158142, 0.12759196758270264, 0.40325498580932617, 0.08119088411331177, 0.40554356575012207, 0.4658434987068176, 0.43098169565200806]\n",
      "['dataset/new face.4.jpg', 'dataset/gala1.jpg', 'dataset/new face.8.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/new face.7.jpg', 'dataset/User.2.2.jpg', 'dataset/new face.0.jpg', 'dataset/new face.3.jpg', 'dataset/321.jpg', 'dataset/new face.5.jpg', 'dataset/123.jpg', 'dataset/new face.6.jpg', 'dataset/new face.1.jpg', 'dataset/new face.2.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "idx=0\n",
    "                 \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(224, 224),\n",
    "        #flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        rrec = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w] \n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        #face_resize.shape\n",
    "        face_resize = face_resize.reshape(1,224,224,3)\n",
    "        text = verifyFace(face_resize)\n",
    "        text=str(text)\n",
    "        if text=='donot':\n",
    "            \n",
    "            write_name = 'dataset/new face.'+str(idx)+'.jpg'\n",
    "            cv2.imwrite(write_name, face)\n",
    "            idx+=1\n",
    "        cv2.putText(rrec,text,(100,100), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),1,cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "epsilon=0.2\n",
    "images=glob.glob('dataset/*.jpg')\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "def verifyFace(img1):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n",
    "    \n",
    "    cosines=[]\n",
    "    faces=[]\n",
    "    for i in range(len(images)):\n",
    "        img2=cv2.imread(images[i])\n",
    "        img2=cv2.resize(img2, (width, height))\n",
    "        img2.shape\n",
    "        img2=img2.reshape(1,224,224,3)\n",
    "        img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "\n",
    "        cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "        cosines.append(cosine_similarity)\n",
    "        \n",
    "        #print(cosines)\n",
    "        #print(images)\n",
    "        \n",
    "            \n",
    "    #print(minimum)\n",
    "    minimum = cosines.index(min(cosines))\n",
    "    #print(cosines[minimum])\n",
    "    print(cosines)\n",
    "    print(images)\n",
    "    #print('--------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    if float(cosines[minimum])<0.2:\n",
    "        return images[minimum]\n",
    "            \n",
    "    if float(cosines[minimum])>0.2:\n",
    "        return 'donot'\n",
    "    \n",
    "#return res\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    #if(cosine_similarity < epsilon):\n",
    "     #   print(cosine_similarity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2347462773323059, 0.5857961177825928, 0.41034621000289917, 0.22024130821228027, 0.1422436237335205, 0.0862511396408081]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23466253280639648, 0.5858205556869507, 0.4099884629249573, 0.22009211778640747, 0.14229649305343628, 0.08657091856002808]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23458731174468994, 0.5858084559440613, 0.409792959690094, 0.22003644704818726, 0.1423349380493164, 0.08675980567932129]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.2345280647277832, 0.5858409702777863, 0.4096517562866211, 0.2199879288673401, 0.14235544204711914, 0.08688229322433472]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23447757959365845, 0.5858528017997742, 0.4095519185066223, 0.21995490789413452, 0.14237195253372192, 0.08697402477264404]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23444610834121704, 0.5858509838581085, 0.40949583053588867, 0.219940185546875, 0.14239215850830078, 0.08705174922943115]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23441773653030396, 0.5858573615550995, 0.40943485498428345, 0.21991807222366333, 0.14239686727523804, 0.08710569143295288]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.234397292137146, 0.5858610272407532, 0.40938252210617065, 0.2198963165283203, 0.14239764213562012, 0.08713918924331665]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23439043760299683, 0.5858696699142456, 0.40933752059936523, 0.21987450122833252, 0.14238715171813965, 0.08714914321899414]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.234380304813385, 0.5858680307865143, 0.40930432081222534, 0.219862699508667, 0.14238595962524414, 0.08716785907745361]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.2343776822090149, 0.5858696103096008, 0.4092816710472107, 0.21985167264938354, 0.14238059520721436, 0.08717203140258789]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23437154293060303, 0.5858713090419769, 0.4092642664909363, 0.2198445200920105, 0.14237886667251587, 0.08717983961105347]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.2343653440475464, 0.5858711898326874, 0.40925538539886475, 0.21984225511550903, 0.14238077402114868, 0.08718955516815186]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23436307907104492, 0.5858697295188904, 0.40924668312072754, 0.21983885765075684, 0.14238017797470093, 0.08719277381896973]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23435968160629272, 0.5858737826347351, 0.40923619270324707, 0.21983247995376587, 0.14237797260284424, 0.08719336986541748]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23435699939727783, 0.5858811438083649, 0.4092256426811218, 0.21982413530349731, 0.14237380027770996, 0.08719170093536377]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23435485363006592, 0.5858867466449738, 0.40921467542648315, 0.2198154330253601, 0.14236879348754883, 0.08718740940093994]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23435384035110474, 0.5858910977840424, 0.40920817852020264, 0.21981030702590942, 0.1423649787902832, 0.08718371391296387]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23435360193252563, 0.5858938097953796, 0.4092004895210266, 0.21980589628219604, 0.14236027002334595, 0.08717876672744751]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n",
      "[0.23738276958465576, 0.5466304123401642, 0.39098894596099854, 0.25683438777923584, 0.17554014921188354, 0.12634968757629395]\n",
      "['dataset/gala1.jpg', 'dataset/0004_01.jpg', 'dataset/Shamil.jpg', 'dataset/User.2.2.jpg', 'dataset/321.jpg', 'dataset/123.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "(width, height) = (224, 224)\n",
    "idx=0\n",
    "                 \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(224, 224)\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        rrec = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w] \n",
    "        \n",
    "    face_resize = cv2.resize(face, (width, height))\n",
    "    face_resize = face_resize.reshape(1,224,224,3)\n",
    "    funct = func(face_resize)\n",
    "    text='Kuanysh'\n",
    "    cv2.putText(rrec,text,(x,y), cv2.FONT_HERSHEY_SIMPLEX, 1,(167,34,56),1,cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "epsilon=0.2\n",
    "images=glob.glob('dataset/*.jpg')\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "def func(img1):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n",
    "    \n",
    "    cosines=[]\n",
    "    faces=[]\n",
    "    for i in range(len(images)):\n",
    "        img2=cv2.imread(images[i])\n",
    "        img2=cv2.resize(img2, (width, height))\n",
    "        img2.shape\n",
    "        img2=img2.reshape(1,224,224,3)\n",
    "        img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "\n",
    "        cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "        cosines.append(cosine_similarity)\n",
    "    print(cosines)\n",
    "    print(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
