{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can download the pretrained weights from the following link \n",
    "#https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "#or you can find the detailed documentation https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "\n",
    "from keras.models import model_from_json\n",
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    #img = load_img(image_path, target_size=(224, 224))\n",
    "    #img = img_to_array(img)\n",
    "    #img = np.expand_dims(image_path, axis=0)\n",
    "    img = preprocess_input(image_path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2953547239303589\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.03470110893249512\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.012829363346099854\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.020385563373565674\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.03111577033996582\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.040417492389678955\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.02755528688430786\n",
      "dataset/new_face.0.jpg\n",
      "-----------------------------\n",
      "0.10518282651901245\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.09411978721618652\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.10860228538513184\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.09581100940704346\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.09093517065048218\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.07239365577697754\n",
      "dataset/123.jpg\n",
      "-----------------------------\n",
      "0.06612032651901245\n",
      "dataset/123.jpg\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "threshold=0.2\n",
    "(width, height) = (224, 224)\n",
    "idx=0\n",
    "                 \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(224, 224)\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        rrec = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w] \n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        face_resize = face_resize.reshape(1,224,224,3)\n",
    "        img1_representation = vgg_face_descriptor.predict(preprocess_image(face_resize))[0,:]\n",
    "        \n",
    "        images=glob.glob('dataset/*.jpg')\n",
    "        cosines=[]\n",
    "        faces=[]\n",
    "        for i in range(len(images)):\n",
    "            img2=cv2.imread(images[i])\n",
    "            img2=cv2.resize(img2, (width, height))\n",
    "            img2=img2.reshape(1,224,224,3)\n",
    "            \n",
    "            img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "\n",
    "            cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "            cosines.append(cosine_similarity)\n",
    "        \n",
    "        minimum = cosines.index(min(cosines))\n",
    "        #percent = (0.4-float(cosines[minimum]))*100/0.4\n",
    "        #print(str(percent)+'%')\n",
    "        print(cosines[minimum])\n",
    "        print(images[minimum])\n",
    "        print('-----------------------------')\n",
    "        \n",
    "        if float(cosines[minimum])<0.2:\n",
    "            text=str(images[minimum])\n",
    "            \n",
    "        if float(cosines[minimum])>0.2:\n",
    "            text='not found'\n",
    "            write_name = 'dataset/new_face.'+str(idx)+'.jpg'\n",
    "            cv2.imwrite(write_name, face)\n",
    "            idx+=1\n",
    "        \n",
    "        cv2.putText(rrec,text,(x,y), cv2.FONT_HERSHEY_SIMPLEX, 1,(167,34,56),1,cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "epsilon=0.2\n",
    "images=glob.glob('dataset/*.jpg')\n",
    "(width, height) = (224, 224)\n",
    "\n",
    "def func(img1):\n",
    "    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n",
    "    \n",
    "    cosines=[]\n",
    "    faces=[]\n",
    "    for i in range(len(images)):\n",
    "        img2=cv2.imread(images[i])\n",
    "        img2=cv2.resize(img2, (width, height))\n",
    "        img2.shape\n",
    "        img2=img2.reshape(1,224,224,3)\n",
    "        img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n",
    "\n",
    "        cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "        cosines.append(cosine_similarity)\n",
    "    print(cosines)\n",
    "    print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
